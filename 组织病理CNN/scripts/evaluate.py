"""
æ¨¡å‹è¯„ä¼°è„šæœ¬

ç”¨äºå…¨é¢è¯„ä¼°ç»„ç»‡ç—…ç†CNNæ¨¡å‹çš„æ€§èƒ½
"""

import os
import sys
import torch
import numpy as np
import argparse
import json
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ srcè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from src.data import PathologyDataLoader
from src.models import ModelManager
from src.training import MetricsCalculator
from src.utils import VisualizationUtils
from configs.config import Config

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        model_path: str = None,
        data_dir: str = None,
        device: str = "auto",
        batch_size: int = 32
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            data_dir: æµ‹è¯•æ•°æ®ç›®å½•
            device: è®¾å¤‡ç±»å‹
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        # è®¾å¤‡é…ç½®
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.model_manager = ModelManager()
        self.model, self.model_info = self.model_manager.load_model(
            model_path=model_path,
            load_best=True,
            device=self.device
        )
        
        self.model.eval()
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_info.get('model_type', 'unknown')}")
        
        # åŠ è½½æ•°æ®
        if data_dir and os.path.exists(data_dir):
            self.data_loader = PathologyDataLoader(
                data_dir=data_dir,
                batch_size=batch_size
            )
            self.test_loader = self.data_loader.get_test_loader()
            self.val_loader = self.data_loader.get_val_loader()
            print(f"æ•°æ®åŠ è½½æˆåŠŸ: æµ‹è¯•æ ·æœ¬ {len(self.test_loader.dataset)}")
        else:
            self.test_loader = None
            self.val_loader = None
            print("æœªæä¾›æµ‹è¯•æ•°æ®ï¼Œä»…æ”¯æŒåŸºäºå·²æœ‰æŒ‡æ ‡çš„è¯„ä¼°")
        
        # è¯„ä¼°å·¥å…·
        self.metrics_calculator = MetricsCalculator()
        self.class_names = Config.PATHOLOGY_CLASSES
        
    def evaluate_test_set(self) -> Dict[str, any]:
        """è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½"""
        if self.test_loader is None:
            raise ValueError("æœªæä¾›æµ‹è¯•æ•°æ®åŠ è½½å™¨")
        
        print("\nğŸ§ª å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        
        all_predictions = []
        all_labels = []
        all_probabilities = []
        
        with torch.no_grad():
            for batch_idx, (data, targets) in enumerate(self.test_loader):
                data, targets = data.to(self.device), targets.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                
                # æ”¶é›†ç»“æœ
                all_predictions.extend(preds.cpu().numpy())
                all_labels.extend(targets.cpu().numpy())
                all_probabilities.extend(probs.cpu().numpy())
                
                if batch_idx % 10 == 0:
                    print(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{len(self.test_loader)}")
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.metrics_calculator.calculate_metrics(
            all_labels, all_predictions, all_probabilities
        )
        
        # è¯¦ç»†åˆ†æ
        detailed_results = {
            "basic_metrics": metrics,
            "per_class_metrics": self.metrics_calculator.calculate_per_class_metrics(
                all_labels, all_predictions
            ),
            "confusion_matrix": self.metrics_calculator.get_confusion_matrix(
                all_labels, all_predictions
            ).tolist(),
            "classification_report": self.metrics_calculator.get_classification_report(
                all_labels, all_predictions
            ),
            "predictions_summary": {
                "total_samples": len(all_labels),
                "correct_predictions": sum(1 for pred, true in zip(all_predictions, all_labels) if pred == true),
                "prediction_accuracy": sum(1 for pred, true in zip(all_predictions, all_labels) if pred == true) / len(all_labels)
            }
        }
        
        return detailed_results
    
    def evaluate_model_complexity(self) -> Dict[str, any]:
        """è¯„ä¼°æ¨¡å‹å¤æ‚åº¦"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹å¤æ‚åº¦...")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        param_size = 0
        buffer_size = 0
        
        for param in self.model.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in self.model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        model_size_mb = (param_size + buffer_size) / 1024 / 1024
        
        # æ¨ç†é€Ÿåº¦æµ‹è¯•
        inference_times = []
        test_input = torch.randn(1, 3, Config.IMG_SIZE, Config.IMG_SIZE).to(self.device)
        
        # é¢„çƒ­
        for _ in range(10):
            with torch.no_grad():
                _ = self.model(test_input)
        
        # æµ‹è¯•æ¨ç†æ—¶é—´
        with torch.no_grad():
            for _ in range(100):
                start_time = torch.cuda.Event(enable_timing=True)
                end_time = torch.cuda.Event(enable_timing=True)
                
                start_time.record()
                _ = self.model(test_input)
                end_time.record()
                
                torch.cuda.synchronize()
                inference_times.append(start_time.elapsed_time(end_time))
        
        avg_inference_time = np.mean(inference_times)
        
        complexity_info = {
            "model_parameters": {
                "total": total_params,
                "trainable": trainable_params,
                "non_trainable": total_params - trainable_params
            },
            "model_size_mb": model_size_mb,
            "inference_performance": {
                "avg_inference_time_ms": avg_inference_time,
                "fps": 1000 / avg_inference_time,
                "samples_processed": 100
            },
            "model_info": self.model_info
        }
        
        return complexity_info
    
    def evaluate_class_balance(self) -> Dict[str, any]:
        """è¯„ä¼°ç±»åˆ«å¹³è¡¡æ€§"""
        if self.test_loader is None:
            print("âš ï¸  æ— æ³•è¯„ä¼°ç±»åˆ«å¹³è¡¡ï¼šæ— æµ‹è¯•æ•°æ®")
            return {}
        
        print("\nâš–ï¸ è¯„ä¼°ç±»åˆ«å¹³è¡¡æ€§...")
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        class_counts = {class_name: 0 for class_name in self.class_names}
        
        for _, targets in self.test_loader:
            for target in targets:
                class_name = self.class_names[target.item()]
                class_counts[class_name] += 1
        
        total_samples = sum(class_counts.values())
        class_percentages = {k: v/total_samples*100 for k, v in class_counts.items()}
        
        # è®¡ç®—å¹³è¡¡æ€§æŒ‡æ ‡
        class_counts_list = list(class_counts.values())
        balance_ratio = max(class_counts_list) / min(class_counts_list) if min(class_counts_list) > 0 else float('inf')
        
        balance_info = {
            "class_distribution": class_counts,
            "class_percentages": class_percentages,
            "balance_ratio": balance_ratio,
            "total_samples": total_samples,
            "is_balanced": balance_ratio < 2.0,
            "recommendation": self._get_balance_recommendation(balance_ratio)
        }
        
        return balance_info
    
    def generate_evaluation_report(
        self,
        save_plots: bool = True,
        output_dir: str = "evaluation_results"
    ) -> Dict[str, any]:
        """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "model_info": self.model_info,
            "evaluation_settings": {
                "device": self.device,
                "class_names": self.class_names
            }
        }
        
        # åŸºç¡€æ€§èƒ½è¯„ä¼°
        if self.test_loader:
            print("ğŸ” æ‰§è¡Œæ€§èƒ½è¯„ä¼°...")
            performance_results = self.evaluate_test_set()
            report["performance_evaluation"] = performance_results
            
            # ç”Ÿæˆå¯è§†åŒ–
            if save_plots:
                print("ğŸ“ˆ ç”Ÿæˆæ€§èƒ½å›¾è¡¨...")
                
                # æ··æ·†çŸ©é˜µ
                cm = np.array(performance_results["confusion_matrix"])
                fig_cm = self.metrics_calculator.plot_confusion_matrix(
                    performance_results["predictions_summary"]["total_samples"] - 1,  # å ä½ç¬¦
                    performance_results["predictions_summary"]["total_samples"] - 1,  # å ä½ç¬¦
                    save_path=os.path.join(output_dir, f"confusion_matrix_{timestamp}.png")
                )
                plt.close(fig_cm)
                
                # åˆ†ç±»æŠ¥å‘Šçƒ­å›¾
                self._plot_classification_heatmap(
                    performance_results["per_class_metrics"],
                    save_path=os.path.join(output_dir, f"classification_heatmap_{timestamp}.png")
                )
        
        # æ¨¡å‹å¤æ‚åº¦è¯„ä¼°
        print("ğŸ—ï¸ æ‰§è¡Œå¤æ‚åº¦è¯„ä¼°...")
        complexity_results = self.evaluate_model_complexity()
        report["complexity_evaluation"] = complexity_results
        
        # ç±»åˆ«å¹³è¡¡æ€§è¯„ä¼°
        print("âš–ï¸ æ‰§è¡Œå¹³è¡¡æ€§è¯„ä¼°...")
        balance_results = self.evaluate_class_balance()
        report["balance_evaluation"] = balance_results
        
        # æ€»ä½“è¯„ä¼°å’Œå»ºè®®
        print("ğŸ’¡ ç”Ÿæˆè¯„ä¼°å»ºè®®...")
        report["overall_assessment"] = self._generate_overall_assessment(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(output_dir, f"evaluation_report_{timestamp}.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°æ‘˜è¦
        self._print_evaluation_summary(report)
        
        return report
    
    def _plot_classification_heatmap(
        self, 
        per_class_metrics: Dict[str, Dict[str, float]], 
        save_path: str = None
    ):
        """ç»˜åˆ¶åˆ†ç±»æ€§èƒ½çƒ­å›¾"""
        # å‡†å¤‡æ•°æ®
        classes = list(per_class_metrics.keys())
        metrics = ['precision', 'recall', 'f1']
        
        data_matrix = []
        for class_name in classes:
            class_metrics = per_class_metrics[class_name]
            row = [class_metrics[metric] for metric in metrics]
            data_matrix.append(row)
        
        # åˆ›å»ºçƒ­å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            data_matrix,
            xticklabels=metrics,
            yticklabels=classes,
            annot=True,
            fmt='.3f',
            cmap='YlOrRd',
            cbar_kws={'label': 'Score'}
        )
        plt.title('Per-Class Classification Performance')
        plt.xlabel('Metrics')
        plt.ylabel('Classes')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def _generate_overall_assessment(self, report: Dict[str, any]) -> Dict[str, any]:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        assessment = {
            "strengths": [],
            "weaknesses": [],
            "recommendations": [],
            "overall_score": 0.0
        }
        
        # æ€§èƒ½è¯„ä¼°
        if "performance_evaluation" in report:
            perf = report["performance_evaluation"]["basic_metrics"]
            
            if perf["macro_f1"] > 0.85:
                assessment["strengths"].append(f"ä¼˜ç§€çš„F1åˆ†æ•°: {perf['macro_f1']:.3f}")
                assessment["overall_score"] += 30
            elif perf["macro_f1"] > 0.75:
                assessment["strengths"].append(f"è‰¯å¥½çš„F1åˆ†æ•°: {perf['macro_f1']:.3f}")
                assessment["overall_score"] += 20
            else:
                assessment["weaknesses"].append(f"F1åˆ†æ•°éœ€è¦æ”¹è¿›: {perf['macro_f1']:.3f}")
                assessment["recommendations"].append("è€ƒè™‘å¢åŠ è®­ç»ƒæ•°æ®æˆ–è°ƒæ•´æ¨¡å‹æ¶æ„")
        
        # å¤æ‚åº¦è¯„ä¼°
        if "complexity_evaluation" in report:
            comp = report["complexity_evaluation"]
            
            if comp["inference_performance"]["fps"] > 100:
                assessment["strengths"].append(f"é«˜æ¨ç†é€Ÿåº¦: {comp['inference_performance']['fps']:.1f} FPS")
                assessment["overall_score"] += 15
            elif comp["inference_performance"]["fps"] > 50:
                assessment["overall_score"] += 10
            
            if comp["model_size_mb"] < 100:
                assessment["strengths"].append(f"æ¨¡å‹å¤§å°é€‚ä¸­: {comp['model_size_mb']:.1f} MB")
                assessment["overall_score"] += 10
        
        # å¹³è¡¡æ€§è¯„ä¼°
        if "balance_evaluation" in report and report["balance_evaluation"]:
            balance = report["balance_evaluation"]
            
            if balance["is_balanced"]:
                assessment["strengths"].append("æ•°æ®åˆ†å¸ƒå¹³è¡¡")
                assessment["overall_score"] += 15
            else:
                assessment["weaknesses"].append("æ•°æ®åˆ†å¸ƒä¸å¹³è¡¡")
                assessment["recommendations"].append("è€ƒè™‘ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–æ•°æ®é‡é‡‡æ ·")
        
        # æ·»åŠ é€šç”¨å»ºè®®
        if assessment["overall_score"] < 50:
            assessment["recommendations"].append("å»ºè®®å…¨é¢æ£€æŸ¥æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹")
        
        return assessment
    
    def _print_evaluation_summary(self, report: Dict[str, any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°æŠ¥å‘Šæ‘˜è¦")
        print("="*60)
        
        # æ€§èƒ½æ‘˜è¦
        if "performance_evaluation" in report:
            perf = report["performance_evaluation"]["basic_metrics"]
            print(f"\nğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å‡†ç¡®ç‡: {perf['accuracy']:.3f}")
            print(f"   F1åˆ†æ•°: {perf['macro_f1']:.3f}")
            print(f"   ç²¾ç¡®ç‡: {perf['macro_precision']:.3f}")
            print(f"   å¬å›ç‡: {perf['macro_recall']:.3f}")
        
        # å¤æ‚åº¦æ‘˜è¦
        if "complexity_evaluation" in report:
            comp = report["complexity_evaluation"]
            print(f"\nğŸ—ï¸ æ¨¡å‹å¤æ‚åº¦:")
            print(f"   å‚æ•°æ•°é‡: {comp['model_parameters']['total']:,}")
            print(f"   æ¨¡å‹å¤§å°: {comp['model_size_mb']:.1f} MB")
            print(f"   æ¨ç†é€Ÿåº¦: {comp['inference_performance']['fps']:.1f} FPS")
        
        # æ€»ä½“è¯„ä¼°
        if "overall_assessment" in report:
            assess = report["overall_assessment"]
            print(f"\nğŸ’¡ æ€»ä½“è¯„ä¼°:")
            print(f"   æ€»ä½“å¾—åˆ†: {assess['overall_score']}/100")
            
            if assess["strengths"]:
                print("   ä¼˜åŠ¿:")
                for strength in assess["strengths"]:
                    print(f"     âœ… {strength}")
            
            if assess["weaknesses"]:
                print("   åŠ£åŠ¿:")
                for weakness in assess["weaknesses"]:
                    print(f"     âš ï¸  {weakness}")
            
            if assess["recommendations"]:
                print("   å»ºè®®:")
                for rec in assess["recommendations"]:
                    print(f"     ğŸ’¡ {rec}")
        
        print("\n" + "="*60)
    
    def _get_balance_recommendation(self, balance_ratio: float) -> str:
        """è·å–å¹³è¡¡æ€§å»ºè®®"""
        if balance_ratio < 1.5:
            return "æ•°æ®åˆ†å¸ƒè‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†"
        elif balance_ratio < 3.0:
            return "å»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–æ•°æ®å¢å¼º"
        else:
            return "å¼ºçƒˆå»ºè®®é‡æ–°å¹³è¡¡æ•°æ®é›†"

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç»„ç»‡ç—…ç†CNNæ¨¡å‹è¯„ä¼°")
    
    parser.add_argument("--model_path", type=str, 
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼‰")
    parser.add_argument("--data_dir", type=str, 
                       help="æµ‹è¯•æ•°æ®ç›®å½•")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ç±»å‹")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--output_dir", type=str, default="evaluation_results",
                       help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--save_plots", action="store_true", default=True,
                       help="ä¿å­˜å¯è§†åŒ–å›¾è¡¨")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        model_path=args.model_path,
        data_dir=args.data_dir,
        device=args.device,
        batch_size=args.batch_size
    )
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report = evaluator.generate_evaluation_report(
        save_plots=args.save_plots,
        output_dir=args.output_dir
    )

if __name__ == "__main__":
    main()